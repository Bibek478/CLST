{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Impurity Cleanup\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Config\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "input_path = \"CLST.tsv\"\n",
        "\n",
        "# Regex ranges:\n",
        "DEVANAGARI = r'[\\u0900-\\u097F]'   # Hindi script\n",
        "ASSAMESE   = r'[\\u0980-\\u09FF]'   # Assamese script\n",
        "\n",
        "patterns = [\n",
        "    r'\\?{2,}',      # multiple question marks\n",
        "    r'\\.{3,}',      # multiple dots\n",
        "    r'!{2,}',       # multiple exclamation points\n",
        "    r'<[^>]*>?',    # any tags\n",
        "    r'�',           # replacement char\n",
        "]\n",
        "\n",
        "def has_impurity(text):\n",
        "    return any(re.search(p, str(text)) for p in patterns)\n",
        "\n",
        "def clean_text(text):\n",
        "    out = str(text)\n",
        "    for p in patterns:\n",
        "        out = re.sub(p, \"\", out)\n",
        "    # Remove leading and trailing double quotes\n",
        "    out = out.strip().strip('\"')\n",
        "    return out\n",
        "\n",
        "# LOAD (one‑split to avoid concat issues)\n",
        "rows = []\n",
        "with open(input_path, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        a, b = (line.rstrip(\"\\n\").split(\"\\t\", 1) + [\"\"])[:2]\n",
        "        rows.append((a, b))\n",
        "df = pd.DataFrame(rows, columns=[\"asm\",\"eng\"])\n",
        "print(f\"Loaded {len(df)} rows\\n\")\n",
        "\n",
        "# DETECT & CLEAN PUNCTUATION IMPURITIES\n",
        "mask_impure = df['asm'].apply(has_impurity) | df['eng'].apply(has_impurity)\n",
        "print(f\"Impure rows (by punctuation): {mask_impure.sum()}\")\n",
        "\n",
        "print(\"\\n--- BEFORE cleaning ---\")\n",
        "display(df.loc[mask_impure, [\"asm\",\"eng\"]])\n",
        "\n",
        "df.loc[mask_impure, \"asm\"] = df.loc[mask_impure, \"asm\"].apply(clean_text)\n",
        "df.loc[mask_impure, \"eng\"] = df.loc[mask_impure, \"eng\"].apply(clean_text)\n",
        "\n",
        "print(\"\\n--- AFTER cleaning ---\")\n",
        "display(df.loc[mask_impure, [\"asm\",\"eng\"]])\n",
        "\n",
        "\n",
        "# SHOW mixed‐script in English (for review only)\n",
        "mask_mixed_eng = df['eng'].str.contains(ASSAMESE, regex=True) | df['eng'].str.contains(DEVANAGARI, regex=True)\n",
        "print(f\"\\nRows where English column contains Assamese or Hindi script: {mask_mixed_eng.sum()}\")\n",
        "display(df.loc[mask_mixed_eng, [\"asm\",\"eng\"]])"
      ],
      "metadata": {
        "id": "_lS5SKhpiv7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Remove Duplicates\n",
        "# Show exact duplicates (including both source and target)\n",
        "duplicates = df[df.duplicated()]\n",
        "print(f\"Found {len(duplicates)} exact duplicate rows:\")\n",
        "display(duplicates)\n",
        "\n",
        "# Save the duplicates for backup or audit if needed\n",
        "duplicates.to_csv(\"duplicates_removed.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Remove the exact duplicate rows\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nRemoved {len(duplicates)} duplicate rows. Remaining rows: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "m6ailTUq_AtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with NaN in either 'asm' or 'eng' column\n",
        "df = df.dropna(subset=['asm', 'eng']).reset_index(drop=True)\n",
        "\n",
        "print(f\"After dropping NaN rows: {len(df)} rows\")"
      ],
      "metadata": {
        "id": "e9o-TCo8iLD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Remove empty rows\n",
        "\n",
        "# Remove rows where either column is completely empty or just whitespace\n",
        "mask_empty = (df['asm'].str.strip() == \"\") | (df['eng'].str.strip() == \"\")\n",
        "print(f\"Removing {mask_empty.sum()} rows where either column is empty.\")\n",
        "\n",
        "# Drop them\n",
        "df = df[~mask_empty].reset_index(drop=True)\n",
        "\n",
        "print(f\"Remaining rows after empty-row removal: {len(df)}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CCFTV4KYkcag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Outlier Filtering by Word‐Count Ratio\n",
        "\n",
        "import re\n",
        "\n",
        "# 1) Define a word‐count function\n",
        "def word_count(text):\n",
        "    # find all alphanumeric “words”\n",
        "    return len(re.findall(r'\\w+', str(text)))\n",
        "\n",
        "# 2) Compute word counts for source and target\n",
        "df['words_asm'] = df['asm'].apply(word_count)\n",
        "df['words_eng'] = df['eng'].apply(word_count)\n",
        "\n",
        "# 3) Compute ratio\n",
        "df['ratio'] = df['words_eng'] / df['words_asm'].replace(0, 1)\n",
        "\n",
        "# 4) Define acceptable ratio range\n",
        "low, high = 0.2, 5.0\n",
        "mask_ratio = (df['ratio'] < low) | (df['ratio'] > high)\n",
        "\n",
        "# 5) Show how many outliers\n",
        "print(\"Outlier rows count:\", mask_ratio.sum())\n",
        "\n",
        "# 6) Display the actual sentence pairs that are outliers, with their word counts and ratio\n",
        "outliers_df = df[mask_ratio]\n",
        "display(\n",
        "    outliers_df[[\n",
        "        'asm',        # Assamese sentence\n",
        "        'eng',        # English sentence\n",
        "        'words_asm',\n",
        "        'words_eng',\n",
        "        'ratio'\n",
        "    ]]\n",
        ")\n",
        "\n",
        "# 7) Once you’ve inspected them, drop those rows\n",
        "df = df[~mask_ratio].reset_index(drop=True)\n",
        "print(f\"After ratio filtering: {len(df)} rows remaining\")\n"
      ],
      "metadata": {
        "id": "cr4w55jp_GEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create length columns\n",
        "df['len_asm'] = df['asm'].apply(lambda x: len(str(x).split()))\n",
        "df['len_eng'] = df['eng'].apply(lambda x: len(str(x).split()))"
      ],
      "metadata": {
        "id": "eOUY3cIkgVJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1) Make sure length columns exist\n",
        "df['len_asm'] = df['asm'].apply(lambda x: len(str(x).split()))\n",
        "df['len_eng'] = df['eng'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# 2) Determine the full max length in your data\n",
        "max_len = max(df['len_asm'].max(), df['len_eng'].max())\n",
        "print(f\"Maximum sentence length in tokens: {max_len}\")\n",
        "\n",
        "# 3) Create bins for every integer length from 0 to max_len\n",
        "bins = np.arange(0, max_len + 2)\n",
        "\n",
        "# 4) Decide tick spacing so labels don’t crowd\n",
        "tick_step = max(1, max_len // 20)\n",
        "ticks = np.arange(0, max_len + 1, tick_step)\n",
        "\n",
        "# 5) Plot side-by-side histograms\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "# Assamese\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['len_asm'], bins=bins, edgecolor='black')\n",
        "plt.title('Assamese Sentence Lengths')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Number of Sentences')\n",
        "plt.xticks(ticks, rotation=45)\n",
        "plt.xlim(0, max_len + 1)\n",
        "\n",
        "# English\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df['len_eng'], bins=bins, color='orange', edgecolor='black')\n",
        "plt.title('English Sentence Lengths')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Number of Sentences')\n",
        "plt.xticks(ticks, rotation=45)\n",
        "plt.xlim(0, max_len + 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IqWX2UHPk7-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        min_token_length_str = input(\"Enter the MINIMUM token length you want to keep for sentences (e.g., 10): \")\n",
        "        min_token_length = int(min_token_length_str)\n",
        "        if min_token_length < 0:\n",
        "            print(\"Please enter a non-negative integer for minimum length.\")\n",
        "            continue\n",
        "\n",
        "        max_token_length_str = input(\"Enter the MAXIMUM token length you want to keep for sentences (e.g., 100): \")\n",
        "        max_token_length = int(max_token_length_str)\n",
        "        if max_token_length <= 0:\n",
        "            print(\"Please enter a positive integer for maximum length.\")\n",
        "            continue\n",
        "\n",
        "        if min_token_length > max_token_length:\n",
        "            print(f\"Error: Minimum length ({min_token_length}) cannot be greater than maximum length ({max_token_length}). Please try again.\")\n",
        "        else:\n",
        "            break\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter integer numbers for both lengths.\")\n",
        "\n",
        "print(f\"\\nFiltering sentences to keep those with both Assamese and English lengths between {min_token_length} and {max_token_length} tokens (inclusive).\")\n",
        "\n",
        "keep_mask = (df['len_asm'] >= min_token_length) & \\\n",
        "            (df['len_asm'] <= max_token_length) & \\\n",
        "            (df['len_eng'] >= min_token_length) & \\\n",
        "            (df['len_eng'] <= max_token_length)\n",
        "\n",
        "# Filtered DataFrame: Contains sentences that meet the length criteria\n",
        "df_filtered = df[keep_mask].copy()\n",
        "\n",
        "# Removed DataFrame: Contains sentences that were filtered out (i.e., outside the specified range)\n",
        "df_removed = df[~keep_mask].copy()\n",
        "\n",
        "print(f\"\\nOriginal DataFrame size: {len(df)} rows\")\n",
        "print(f\"Filtered DataFrame size (sentences kept within range): {len(df_filtered)} rows\")\n",
        "print(f\"Removed DataFrame size (sentences outside range): {len(df_removed)} rows\")\n",
        "\n",
        "# --- Save to TSV files ---\n",
        "output_filtered_file = f\"filtered_dataset_from_{min_token_length}_to_{max_token_length}_tokens.tsv\"\n",
        "output_removed_file = f\"removed_dataset_outside_{min_token_length}_to_{max_token_length}_tokens.tsv\"\n",
        "output_full_dataset_file = \"complete_dataset_with_token_counts.tsv\" # This file remains the same\n",
        "\n",
        "# --- Prepare DataFrames for Saving with Descriptive Headers ---\n",
        "\n",
        "# For the filtered and removed datasets (only 'asm' and 'eng' columns)\n",
        "df_filtered_output = df_filtered[['asm', 'eng']].rename(columns={\n",
        "    'asm': 'Assamese Sentence',\n",
        "    'eng': 'English Sentence'\n",
        "})\n",
        "df_removed_output = df_removed[['asm', 'eng']].rename(columns={\n",
        "    'asm': 'Assamese Sentence',\n",
        "    'eng': 'English Sentence'\n",
        "})\n",
        "\n",
        "# For the complete dataset (including token counts)\n",
        "df_complete_output = df[['asm', 'eng', 'len_asm', 'len_eng']].rename(columns={\n",
        "    'asm': 'Assamese Sentence',\n",
        "    'eng': 'English Sentence',\n",
        "    'len_asm': 'Assamese Token Count',\n",
        "    'len_eng': 'English Token Count'\n",
        "})\n",
        "\n",
        "# Save the filtered data (sentences within range)\n",
        "df_filtered_output.to_csv(output_filtered_file, sep='\\t', index=False, header=True)\n",
        "print(f\"\\nSaved filtered data (within token range) to: {output_filtered_file}\")\n",
        "\n",
        "# Save the removed data (sentences outside range)\n",
        "df_removed_output.to_csv(output_removed_file, sep='\\t', index=False, header=True)\n",
        "print(f\"Saved removed data (outside token range) to: {output_removed_file}\")\n",
        "\n",
        "# Save the complete dataset with token counts\n",
        "df_complete_output.to_csv(output_full_dataset_file, sep='\\t', index=False, header=True)\n",
        "print(f\"Saved complete dataset with token counts to: {output_full_dataset_file}\")\n",
        "\n",
        "print(\"\\nFiltering and saving complete.\")"
      ],
      "metadata": {
        "id": "_W2tpvNCEkaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}